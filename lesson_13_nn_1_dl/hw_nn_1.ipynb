{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145475eb",
   "metadata": {},
   "source": [
    "# Написать на pytorch глубокую сеть, проверить работу форвардпасса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978461e6",
   "metadata": {},
   "source": [
    "Forward pass (прямой проход) - это процесс, в котором входные данные проходят через нейронную сеть от входного слоя до выходного слоя, с применением весов, активаций и функций активации на каждом слое. В результате происходит преобразование входных данных в выходные, которые могут быть использованы для решения задачи, например, для классификации изображений или предсказания цены акций. Форвардпасс является одним из основных шагов обучения нейронной сети, при котором считаются значения выходного слоя и функции потерь, на основе которых осуществляется обратное распространение ошибки (backpropagation) и обновление весов сети."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58787853",
   "metadata": {},
   "source": [
    "nn.Sigmoid() - это функция активации, которая применяется в нейронных сетях для преобразования взвешенной суммы входных данных в значение от 0 до 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b43083",
   "metadata": {},
   "source": [
    "nn.Linear(1, 100) - это слой нейронной сети, который выполняет линейное преобразование входных данных. Он принимает на вход тензор размерности (batch_size, input_size), где input_size - это количество входных признаков (в данном случае равно 1), а batch_size - это количество примеров в одной батче. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29586a08",
   "metadata": {},
   "source": [
    "nn.ReLU() является функцией активации в нейронных сетях и обычно используется после слоя линейной трансформации (например, nn.Linear). Она применяет нелинейное преобразование к выходу предыдущего слоя, заменяя все отрицательные значения на ноль, а положительные значения оставляет без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f08f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data: tensor([[ 0.9410,  0.4688, -0.3612,  0.6726,  0.9023]])\n",
      "tensor([[0.4682]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Определение архитектуры сети\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 10)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Создание экземпляра\n",
    "net = DeepNN()\n",
    "\n",
    "# net.train()\n",
    "\n",
    "# Создание случайных входных данных\n",
    "input_data = torch.randn(1, 5)\n",
    "print(f'input_data: {input_data}')\n",
    "\n",
    "# Прохождение данных через сеть\n",
    "output_data = net(input_data)\n",
    "\n",
    "# Вывод результата\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1197bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30a69ed4",
   "metadata": {},
   "source": [
    "# Написать адаптивный оптимизатор"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d31c487",
   "metadata": {},
   "source": [
    "Основная идея адаптивных оптимизаторов состоит в том, чтобы адаптировать скорость обучения (learning rate) для каждого параметра модели в зависимости от изменения градиента в процессе обучения. В отличие от простых методов градиентного спуска, где скорость обучения остается постоянной на протяжении всего процесса обучения, адаптивные методы позволяют управлять скоростью обучения для каждого параметра в зависимости от текущего положения в пространстве параметров.\n",
    "В целом, использование адаптивных оптимизаторов может помочь ускорить процесс обучения и достичь лучшей точности модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d90df50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.000999999998"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "#     learning_rate — это скорость обучения\n",
    "        self.learning_rate = learning_rate\n",
    "#     beta1 и beta2 — коэффициенты для экспоненциального сглаживания градиентов и их квадратов\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "#     epsilon — малое число для численной стабильности\n",
    "        self.epsilon = epsilon\n",
    "#     m и v — две переменные для хранения предыдущих значений градиентов и их квадратов\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "#     t — текущая итерация\n",
    "        self.t = 0\n",
    "        \n",
    "    def update(self, w, grad_wrt_w):\n",
    "#         принимает на вход веса w и градиенты grad_wrt_w и возвращает обновленные веса w.\n",
    "        self.t = self.t + 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad_wrt_w\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * np.power(grad_wrt_w, 2)\n",
    "        m_hat = self.m / (1 - np.power(self.beta1, self.t))\n",
    "        v_hat = self.v / (1 - np.power(self.beta2, self.t))\n",
    "        w = w - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return w\n",
    "\n",
    "adam = AdamOptimizer()    \n",
    "adam.update(w=0, grad_wrt_w=5)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4370abf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71f4f690",
   "metadata": {},
   "source": [
    "# Решить задачу нахождения корней квадратного уравнения методом градиентнго спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea232b1",
   "metadata": {},
   "source": [
    "x ** 2 - 6 * x + 4 = 0\n",
    "\n",
    "посчитать производную от преобразованной функции\n",
    "надо начать движение от начальной точки в направлении антиградиента с заданным шагом\n",
    "x = x - lr * grad(x)\n",
    "\n",
    "- всегда ли сойдемся за приемлемое количество шагов?\n",
    "Ответ: Если скорость обучения выбрана слишком большой, то мы можем пропустить минимум функции, а если слишком маленькой - то алгоритм будет сходиться очень медленно.\n",
    "\n",
    "важна ли начальная точка?\n",
    "Ответ: Начальная точка может влиять на итоговый результат, поскольку метод градиентного спуска может застрять в локальном минимуме функции, если начальная точка находится рядом с таким минимумом.\n",
    "\n",
    "как найти второй корень?\n",
    "\n",
    "как влияет ЛР?\n",
    "Ответ: Скорось обучения (learning rate) влияет на то, как быстро алгоритм сходится. Если скорость обучения слишком мала, то алгоритм будет сходиться очень медленно, а если слишком велика - алгоритм может не сходиться вообще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ebf76e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Полином:    2\n",
      "1 x - 6 x + 4\n",
      "Производная:  \n",
      "2 x - 6\n",
      "x: 2.019801346297245\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = 1\n",
    "b = -6\n",
    "c = 4\n",
    "\n",
    "# создаем полиномиальную функцию по ее коэффициентам\n",
    "plnm=np.poly1d([a, b, c])\n",
    "print(f'Полином: {plnm}')\n",
    "\n",
    "# вычисляем производную\n",
    "df = np.polyder(plnm)\n",
    "print(f'Производная: {df}')\n",
    "\n",
    "def grad(x, lr = 0.000001, steps=10000, epsilon=1e-6):\n",
    "    for step in range(steps):\n",
    "        # вычисляем значение производной в текущей точке x\n",
    "        df_dx = np.polyval(df, x)\n",
    "        if abs(df_dx) < epsilon:\n",
    "            break\n",
    "        x = x - lr*df_dx\n",
    "    return x\n",
    "\n",
    "\n",
    "x = grad(x=2)\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008a9af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
