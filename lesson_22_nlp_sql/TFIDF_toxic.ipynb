{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIDF toxic",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfVs3wwd3D3j",
        "outputId": "150e4a5a-2d6d-4c6e-c585-5e7836f3b6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "stemmer = SnowballStemmer('russian')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "from sklearn.metrics.cluster import fowlkes_mallows_score\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from yellowbrick.text import FreqDistVisualizer, TSNEVisualizer\n",
        "\n",
        "from sklearn.pipeline import Pipeline \n",
        "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('labeled.csv', encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "PhHI0tO63F2c"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['toxic'] = df['toxic'].apply(lambda x: 'Toxic' if x==1.0 else 'Not Toxic')"
      ],
      "metadata": {
        "id": "RGVwSlUo4p28"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_df(df):\n",
        "    df = df.str.lower()\n",
        "    df = df.replace(r'[^а-яА-Я]', ' ', regex=True)\n",
        "    df = df.str.strip()\n",
        "    df = df.apply(lambda x:' '.join([word for word in x.split() if word not in stopwords.words('russian') and len(word)>2]))\n",
        "    return df\n",
        "\n",
        "df['comment'] = clean_df(df['comment'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzJYwRcj5myX",
        "outputId": "9b3650ad-3bc0-4fb0-9ab1-7304481055fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "import pymorphy2\n",
        "df = df.dropna(subset=['comment'])\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def lem_tok(text):\n",
        "    text_lem = [morph.parse(word)[0].normal_form for word in text.split(' ')]\n",
        "    return text_lem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNKXS8KG5sRz",
        "outputId": "24ac4d4e-490e-4a00-f47e-fbb4a10c89fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#tfidf = TfidfVectorizer(sublinear_tf=True,\n",
        "                        #ngram_range=(1, 2), \n",
        "                        #tokenizer=lem_tok)\n",
        "\n",
        "# We transform each complaint into a vector\n",
        "#features = tfidf.fit_transform(df['comment'])#.toarray()\n",
        "\n",
        "#labels = df.category_id\n",
        "\n",
        "#print(\"Each of the %d complaints is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), tokenizer=lem_tok)\n",
        "features = tfidf.fit_transform(df.comment).toarray()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpIfS7ZE6A3f",
        "outputId": "acccb981-9371-4db3-f04b-44d46a431cff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 57.2 s, sys: 1.73 s, total: 58.9 s\n",
            "Wall time: 1min 25s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# labels = df.category_id\n",
        "\n",
        "# df['category_id'] = df['toxic'].factorize()[0]\n",
        "# category_id_df = df[['toxic', 'category_id']].drop_duplicates()\n",
        "# # Dictionaries for future use\n",
        "# category_to_id = dict(category_id_df.values)\n",
        "# id_to_category = dict(category_id_df[['category_id', 'toxic']].values)\n",
        "\n",
        "# # New dataframe\n",
        "# df = df.sample(frac=0.2)"
      ],
      "metadata": {
        "id": "kPM114-56jsN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import metrics\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "logreg = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer(sublinear_tf=True,\n",
        "                        ngram_range=(1, 3),\n",
        "                        tokenizer=lem_tok, max_df=1000)),\n",
        "                ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "logreg.fit(df['comment'], df['toxic'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CtxYmEf7Hyy",
        "outputId": "e97843ad-2aba-48b3-fefa-6625ccf91018"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tfidf',\n",
              "                 TfidfVectorizer(max_df=1000, ngram_range=(1, 3),\n",
              "                                 sublinear_tf=True,\n",
              "                                 tokenizer=<function lem_tok at 0x7f59a99747a0>)),\n",
              "                ('clf', LinearSVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WDiC3kKpkfXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c6e164e-5acd-4f9c-f54e-c2196a73e866"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Not Toxic'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "32K9G8wJogvA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0rCsIdeL7UaJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jvq34Nqb7qtP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X-SAa6Ha8TAD"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}