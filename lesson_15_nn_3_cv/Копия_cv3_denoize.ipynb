{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in0PyicHhZDG"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73ieMA485Tme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.MNIST('.', download=True)"
      ],
      "metadata": {
        "id": "SI8UCZuy7hTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhJuBtoz7f43",
        "outputId": "6acf1dc6-669a-408a-cb52-959c72916940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=L size=28x28 at 0x7F77BB660ED0>, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "noized = dataset.data[1234].float() / 255 + torch.normal(\n",
        "    torch.zeros_like(dataset.data[1234].float()), \n",
        "    0.2 * torch.ones_like(dataset.data[1234].float()))\n",
        "plt.imshow(noized.detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0zXXXYP37gFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparams\n",
        "in_chan = 1\n",
        "#TODO change model h_params\n",
        "hidden_ch = 64\n",
        "out_ch = 1\n",
        "device_id = 0\n",
        "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
        "n_epochs = 10\n",
        "batch_size = 128\n",
        "noise_factor = 0.4"
      ],
      "metadata": {
        "id": "uPJauY4hAqJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  #28x28 -> 14x14 -> 7x7 \n",
        "  def __init__(self, in_chan, hidden_ch, out_ch):\n",
        "    super().__init__()\n",
        "    #TODO modify architecture as you wish. Add more layers, make hidden smaller, etc\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass\n",
        "\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  #hidden_out-> hidden -> 28*28 \n",
        "  def __init__(self, in_chan, hidden_ch, out_ch):\n",
        "    super().__init__()\n",
        "    #TODO modify architecture as you wish. Add more layers, make hidden smaller, etc\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    pass\n",
        "\n",
        "    return x\n",
        "\n",
        "class ConvAutoEncoder(nn.Module):\n",
        "  def __init__(self, in_chan, hidden_ch, out_ch):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(in_chan, hidden_ch, out_ch)\n",
        "    self.decoder = Decoder(in_chan, hidden_ch, out_ch)\n",
        "\n",
        "  def forward(self, x):\n",
        "    hidden = self.encoder(x)\n",
        "    x_ = self.decoder(hidden)\n",
        "\n",
        "    return x_\n"
      ],
      "metadata": {
        "id": "KTz2txO4LTZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_conv(data: list):\n",
        "  # data = [(pic, target)...]\n",
        "  pics = []\n",
        "  target = []\n",
        "  for item in data:\n",
        "    pics.append(numpy.array(item[0]))\n",
        "    target.append(item[1])\n",
        "  pics = torch.from_numpy(numpy.array(pics)).float() / 255 # B x W x H\n",
        "  target = torch.from_numpy(numpy.array(target))\n",
        "\n",
        "  return {\n",
        "      'data': pics.unsqueeze(1), # B x 1 x W x H\n",
        "      'target': target.long(),\n",
        "      }"
      ],
      "metadata": {
        "id": "K_PACmDaH8Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_conv = ConvAutoEncoder(in_chan, hidden_ch, out_ch).to(device)\n",
        "optim = torch.optim.Adam(model_conv.parameters())\n",
        "loss_func = nn.MSELoss()"
      ],
      "metadata": {
        "id": "a4gX5zVDIZdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(n_epochs):\n",
        "  dataloader = DataLoader(dataset, \n",
        "                          batch_size, \n",
        "                          shuffle=True, \n",
        "                          collate_fn=collate_fn_conv,\n",
        "                          drop_last = True,\n",
        "                          )\n",
        "  for i, batch in enumerate(dataloader):\n",
        "    optim.zero_grad()\n",
        "    data = batch['data'].to(device)\n",
        "    noized = torch.clamp(data + \n",
        "                         torch.normal(torch.zeros_like(data), \n",
        "                                      noise_factor * torch.ones_like(data)), 0., 1.)\n",
        "    predict = model_conv(noized)\n",
        "    loss = loss_func(predict, data)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if i % 200 == 0:\n",
        "      print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
        "  item = dataset.data[1234].unsqueeze(0).unsqueeze(0).float()/255\n",
        "  item = torch.clamp(item + \n",
        "                     torch.normal(torch.zeros_like(item), \n",
        "                                  noise_factor * torch.ones_like(item)), 0., 1.)\n",
        "  plt.imshow(item.squeeze().cpu().detach().numpy())\n",
        "  plt.show()\n",
        "  plt.imshow(model_conv(item.to(device)).squeeze().cpu().detach().numpy())\n",
        "  plt.show()\n",
        "  torch.save(model_conv.state_dict(), f'./conv_ae_chkpt_conv_{epoch}.pth')"
      ],
      "metadata": {
        "id": "jVX0P0otIk4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9CljFAzIMMEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-57Jq-CW8NmD"
      }
    }
  ]
}